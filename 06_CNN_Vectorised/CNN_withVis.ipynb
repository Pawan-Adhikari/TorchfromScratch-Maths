{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "969b9f96",
   "metadata": {},
   "source": [
    "The goal of this notebook is to visualise individual activation maps and pools from various layer to get a sense of what's happening inside our model. We are essentially trying to white box the model. This is sort of like \"https://adamharley.com/nn_vis/cnn/2d.html\" but for CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b398a1",
   "metadata": {},
   "source": [
    "Firstly the imports!\n",
    "- The entire model runs in numpy python (CPU), but uses vectorised, efficient im2col and pooling operations. \n",
    "- itertools is for specialised iteration for certain operation of tensor class\n",
    "- matplotlib for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5646f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550237f",
   "metadata": {},
   "source": [
    "This is the entire backbone of our model. \n",
    "- Firstly, we have the tensor class. It is a wrapper on numpy array that performs vector operations with child tracking and computational graph.\n",
    "- Secondly the Conv2d is the 2D convolutional layer with im2col operation. \n",
    "- The maxpool2D class is simply the pooling layer class.\n",
    "- FC (Fully Connected) class is the backbone of fully connected layer.\n",
    "\n",
    "Note: We dont have batch normalization yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae29521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensor:\n",
    "    def __init__(self, fromArray=np.zeros((2,2)), _children = (), _operation = ''):\n",
    "        fromArray = fromArray if isinstance(fromArray, np.ndarray) else np.array(fromArray)\n",
    "        #assert len(fromArray.shape) == 2, \"Only 2D Tensors or Scalar to 2D Supported!\"\n",
    "        self.matrix = fromArray\n",
    "        #self.rows = fromArray.shape[0]\n",
    "        #self.columns = fromArray.shape[1]\n",
    "        self.shape = fromArray.shape\n",
    "        self._prev = set(_children)\n",
    "        self._operation = _operation\n",
    "        self._backward = lambda : None\n",
    "        self.grad = None\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor Values = {self.matrix}\"\n",
    "    \n",
    "    @classmethod\n",
    "    def zeros(cls, shape, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = np.zeros(shape, dtype=dtype)\n",
    "        t.shape = shape\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls, shape, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.random.randn(*shape) * 0.1).astype(dtype=dtype)\n",
    "        t.shape = shape\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def he_init(cls, shape, fan_in, dtype=np.float32):\n",
    "        t = tensor()\n",
    "        std = np.sqrt(2.0 / fan_in)\n",
    "        t.matrix = (np.random.randn(*shape) * std).astype(dtype=dtype)\n",
    "        t.shape = shape\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def const(cls, shape, constant=1, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.full(shape, constant)).astype(dtype=dtype)\n",
    "        t.shape = shape\n",
    "        return t\n",
    "    \n",
    "    #Operations\n",
    "    def __add__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix + other.matrix\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(other.matrix) if other.grad is None else other.grad\n",
    "            out1 = self.return_unbroadcasted(out)\n",
    "            out2 = other.return_unbroadcasted(out)\n",
    "            self.grad += out1 #Derivation in the notes. \n",
    "            other.grad += out2\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return self + (-1 * other)\n",
    "    \n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return other + (-1 * other)\n",
    "    \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix * other.matrix\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(out.grad) if other.grad is None else other.grad\n",
    "            out1 = self.return_unbroadcasted(out)\n",
    "            out2 = other.return_unbroadcasted(out)\n",
    "            self.grad += out1* other.matrix #Derivation in the notes. \n",
    "            other.grad += out2 * self.matrix\n",
    "\n",
    "        out = tensor(out_matrix, (self, other), '*')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return self*other\n",
    "    \n",
    "    '''\n",
    "    batch multiplication might cause shape broadcasts.\n",
    "    eg. (3,2,2) @ (1,2,3) = (3,2,3)\n",
    "    this is similar to our element wise operations\n",
    "    thus we should be handling this the same way we did for elementwise operations\n",
    "    But, for now, we would be working in a controlled way (Even for CNNS)\n",
    "    and wouldn't need this handling.\n",
    "    '''\n",
    "    def __matmul__(self, other):\n",
    "        other = other if isinstance(other, tensor) else tensor(other)\n",
    "        assert other.shape[-2] == self.shape[-1], \"Dimension Unsupported for @\"\n",
    "        out_matrix = self.matrix @ other.matrix\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(other.matrix) if other.grad is None else other.grad\n",
    "            self.grad += out.grad @ (other.matrix).swapaxes(-2,-1)#Derivation in the notes.\n",
    "            other.grad += (self.matrix).swapaxes(-2,-1) @ out.grad \n",
    "        out = tensor(out_matrix, (self, other), '@')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "\n",
    "    #I and thus we should learn at this point that to make our class compatible for ND tensors,\n",
    "    #We need the matrix multiplication and Transpose backward to change\n",
    "    #For higher dimensions, matmul = batch matmul where multiplication is done \n",
    "    #along each and every batches of 2D matrix. \n",
    "    #eg. If we have (2,3,3) shape tensor, it implies there are two batches of (3,3) matrices\n",
    "    #similarly, (2,3,3,2) shape = 2x3 batches of 3x2 matrices.\n",
    "    #matrix multiplication, (2,3,3) @ (2,3,2) = (2,3,2)\n",
    "    def swap_axes(self, axis1, axis2):\n",
    "        out_matrix = self.matrix.swapaxes(axis1, axis2)\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad.swapaxes(axis1,axis2)) if self.grad is None else self.grad\n",
    "            self.grad += (out.grad).swapaxes(axis1,axis2) #Not in note, but can be derived similarly.\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), 'T')\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def transpose(self):\n",
    "        out_matrix = self.matrix.transpose()\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad.transpose()) if self.grad is None else self.grad\n",
    "            self.grad += (out.grad).transpose() #Not in note, but can be derived similarly.\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), 'T')\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __rmatmul__(self, other):\n",
    "        other = other if isinstance(other, tensor) else tensor(other)\n",
    "        return other @ self\n",
    "    \n",
    "    def __pow__(self, N):\n",
    "        assert isinstance(N, int | float), \"Can only power up by scalars!\"\n",
    "        out_matrix = self.matrix ** N\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            out1 = self.return_unbroadcasted(out)\n",
    "            self.grad += N * (self.matrix ** (N-1)) * out1\n",
    "        \n",
    "        out = tensor(out_matrix, _children=(self, ), _operation=\"**\")\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return self * (other**-1)\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return other * (self**-1)\n",
    "    \n",
    "    def sum(self):\n",
    "        out_matrix = np.array(([[self.matrix.sum()]]))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += np.ones_like(self.matrix) * out.grad\n",
    "\n",
    "        out = tensor(out_matrix, _children=(self, ), _operation='sum()')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def mean(self):\n",
    "        N = np.prod(self.shape)\n",
    "        out_matrix = np.array(([[self.matrix.sum()/(N)]]))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += np.ones_like(self.matrix) * out.grad / N\n",
    "\n",
    "        out = tensor(out_matrix, _children=(self, ), _operation='mean()')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def ReLU(self):\n",
    "        out_matrix = np.maximum(0,self.matrix)\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += (self.matrix > 0).astype(self.matrix.dtype) * out.grad\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), \"ReLU\")\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def reshape(self, shape):\n",
    "        assert isinstance(shape, tuple), f\"Can only reshape using shape tuples e.g. (3,3). Provided is {shape}\"\n",
    "        out_matrix = self.matrix.reshape(shape)\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += out.grad.reshape(self.shape)\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), \"reshape()\")\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def flatten(self):\n",
    "        out_matrix = self.matrix.reshape(-1,np.prod(self.shape[1:]))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += out.grad.reshape(self.shape)\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), \"flatten()\")\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    #Helper Functions\n",
    "    #def shape(self):\n",
    "     #   return (self.rows, self.columns)\n",
    "\n",
    "    def return_unbroadcasted(self, out):  \n",
    "        added_axis = []\n",
    "        stretched_axis = []\n",
    "        for index, (first_no, second_no) in enumerate(itertools.zip_longest(reversed(self.shape), reversed(out.shape))):\n",
    "            if first_no is None:\n",
    "                added_axis.append(index)\n",
    "            elif (first_no == 1) and (second_no > 1):\n",
    "                stretched_axis.append(index)\n",
    "        grad = out.grad\n",
    "        ndim = len(out.shape)\n",
    "        if stretched_axis:\n",
    "            original_axes = tuple(ndim - 1 - i for i in stretched_axis)\n",
    "            grad = np.sum(grad, axis=original_axes, keepdims=True)\n",
    "        if added_axis:\n",
    "            original_axes = tuple(ndim - 1 - i for i in added_axis)\n",
    "            grad = np.sum(grad, axis=original_axes, keepdims=False)\n",
    "        return grad\n",
    "\n",
    "    def checkOther(self, other):\n",
    "        if isinstance(other, int | float):\n",
    "            other = tensor.const(self.shape, other)\n",
    "        elif not isinstance(other, tensor):\n",
    "            other = tensor(other)\n",
    "        #assert other.shape == self.shape, \"Operand Tensor sizes dont match\"\n",
    "\n",
    "        return other\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad = None\n",
    "        \n",
    "    def backward(self):\n",
    "        self.grad = np.ones_like(self.matrix, dtype=float)\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        for current in reversed(topo):\n",
    "\n",
    "            current._backward()\n",
    "\n",
    "    def cleanBackward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev: build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        for t in topo:\n",
    "            t.grad = None\n",
    "            t._prev = ()\n",
    "            t._backward = lambda: None\n",
    "\n",
    "    def exp(self):\n",
    "        out_matrix = np.exp(self.matrix)\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += out_matrix * out.grad  \n",
    "        \n",
    "        out = tensor(out_matrix, (self,), 'exp')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def log(self, eps=1e-8):\n",
    "        clipped = np.clip(self.matrix, eps, None)  \n",
    "        out_matrix = np.log(clipped)\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += (1.0 / clipped) * out.grad \n",
    "        \n",
    "        out = tensor(out_matrix, (self,), 'log')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def softmax(self, axis=-1):\n",
    "        out_matrix = np.exp(self.matrix) / np.sum(np.exp(self.matrix), axis = axis, keepdims=True)\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += out_matrix*(out.grad - np.sum(out_matrix * out.grad, axis = axis, keepdims=True))\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), 'softmax')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def log_softmax(self, axis=-1):\n",
    "        c = np.max(self.matrix, axis=axis, keepdims=True)\n",
    "\n",
    "        out_matrix = (self.matrix-c) - np.log(np.sum(np.exp(self.matrix-c), axis=axis, keepdims=True))\n",
    "        softmax = np.exp(self.matrix-c) / np.sum(np.exp(self.matrix-c), axis = axis, keepdims=True)\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += out.grad - softmax * np.sum(out.grad, axis = axis, keepdims= True)\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), 'log-softmax')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def padding(self, pad_h, pad_w):\n",
    "        np_padding = ((0, 0), (0, 0), (pad_h, pad_h), (pad_w, pad_w))\n",
    "        out_matrix = np.pad(self.matrix, np_padding, 'constant', constant_values=(0, ))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            h_end = -pad_h if pad_h > 0 else None\n",
    "            w_end = -pad_w if pad_w > 0 else None\n",
    "            self.grad += out.grad[:, :, pad_h:h_end, pad_w:w_end]\n",
    "\n",
    "        out = tensor(out_matrix, _children=(self, ), _operation='pad')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    __array_ufunc__ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f573ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d:\n",
    "    def __init__(self, in_channels=1, out_channels=1, kernel_size=2, stride=1, padding = -1):\n",
    "        #self.kernel = tensor.random((out_channels, in_channels, kernel_size, kernel_size))\n",
    "        fan_in = in_channels * kernel_size * kernel_size\n",
    "        self.kernel = tensor.he_init((out_channels, in_channels, kernel_size, kernel_size), fan_in)\n",
    "        self.bias = tensor.zeros((out_channels, ))\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.kernel, self.bias]\n",
    "\n",
    "    def __call__(self, X : tensor):\n",
    "\n",
    "        batch_size, channels, h, w = X.shape\n",
    "\n",
    "        if self.padding == -1: #same padding\n",
    "            pad_h = (self.stride *(h - 1) + self.kernel_size - h)//2\n",
    "            pad_w = (self.stride*(w - 1) + self.kernel_size - w)//2\n",
    "            X_padded = X.padding(pad_h, pad_w)\n",
    "\n",
    "        elif self.padding > 0:\n",
    "            X_padded = X.padding(self.padding, self.padding)\n",
    "\n",
    "        else:\n",
    "            X_padded = X\n",
    "\n",
    "        X_col, act_h, act_w = Conv2d.im2col(X_padded, kernel_size=self.kernel_size, stride=self.stride)\n",
    "        K_col_shape = (self.out_channels, self.kernel_size*self.kernel_size*self.in_channels)\n",
    "        K_col = self.kernel.reshape(K_col_shape).transpose()\n",
    "        Y_col = X_col @ K_col + self.bias\n",
    "        Y = Y_col.reshape((batch_size, self.out_channels, act_h, act_w))\n",
    "        return Y\n",
    "        \n",
    "    @classmethod\n",
    "    def im2col(cls, X : tensor, kernel_size=2, stride=1):\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "        channels = X.shape[1]\n",
    "        image_height = X.shape[-2] #Rows\n",
    "        image_width = X.shape[-1] #Columns\n",
    "\n",
    "        #We are assuming square kernels.\n",
    "        kernel_h = kernel_size\n",
    "        kernel_w = kernel_size\n",
    "\n",
    "        act_h = (((image_height - kernel_h)//stride) + 1) #height of activation\n",
    "        act_w = (((image_width - kernel_w)//stride) + 1)  #width of activation\n",
    "\n",
    "        istrides = X.matrix.strides #strides of input tensor\n",
    "\n",
    "        intermediate_6D = np.lib.stride_tricks.as_strided(\n",
    "                            X.matrix,\n",
    "                            shape=(batch_size, act_h, act_w, channels, kernel_h, kernel_w),\n",
    "                            strides=(istrides[0], #No of images stride bytes\n",
    "                                     istrides[-2] * stride, #Activation map Vertical stride bytes\n",
    "                                     istrides[-1] * stride, #Activation map Horizontal stride bytes\n",
    "                                     istrides[1], #Channel stride bytes\n",
    "                                     istrides[-2], #Rective field vertical stride bytes\n",
    "                                     istrides[-1]) #Receptive field horizontal stride bytes\n",
    "                            )\n",
    "        \n",
    "        out_shape = (batch_size * act_h * act_w, channels * kernel_h * kernel_w)\n",
    "        out_matrix = np.reshape(intermediate_6D, shape=out_shape)\n",
    "\n",
    "\n",
    "        def _backward():\n",
    "            X.grad = np.zeros_like(X.matrix) if X.grad is None else X.grad\n",
    "            \n",
    "            grad_6D = out.grad.reshape(batch_size, act_h, act_w, channels, kernel_h, kernel_w)\n",
    "            for i in range(kernel_h):\n",
    "                for j in range(kernel_w):\n",
    "                    grad_slice = grad_6D[:, :, :, :, i, j]\n",
    "                    \n",
    "                    grad_slice_transposed = grad_slice.transpose(0, 3, 1, 2)\n",
    "                    X.grad[:, :, \n",
    "                        i : i + act_h * stride : stride, \n",
    "                        j : j + act_w * stride : stride\n",
    "                    ] += grad_slice_transposed\n",
    "\n",
    "        out = tensor(out_matrix, _children=(X, ), _operation='im2col')\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out, act_h, act_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49fb6676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class maxpool2D:\n",
    "    def __init__(self, in_channels, pool_size = 2, stride = 1):\n",
    "        self.in_channels = in_channels\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "    def __call__(self, Y: tensor):\n",
    "\n",
    "        batch_number = Y.shape[0]\n",
    "        filters = Y.shape[1]\n",
    "        image_height = Y.shape[-2] #Rows\n",
    "        image_width = Y.shape[-1] #Columns\n",
    "\n",
    "\n",
    "        #We are assuming square kernels.\n",
    "        pool_h = self.pool_size\n",
    "        pool_w = self.pool_size\n",
    "\n",
    "        pooled_h = (((image_height - pool_h)//self.stride) + 1) #height of activation\n",
    "        pooled_w = (((image_width - pool_w)//self.stride) + 1)  #width of activation\n",
    "\n",
    "        istrides = Y.matrix.strides #strides of input tensor\n",
    "\n",
    "        intermediate_6D = np.lib.stride_tricks.as_strided(\n",
    "                            Y.matrix,\n",
    "                            shape=(batch_number, pooled_h, pooled_w, filters, pool_h, pool_w),\n",
    "                            strides=(istrides[0], #No of images stride bytes\n",
    "                                     istrides[-2] * self.stride, #Activation map Vertical stride bytes\n",
    "                                     istrides[-1] * self.stride, #Activation map Horizontal stride bytes\n",
    "                                     istrides[1], #Channel stride bytes\n",
    "                                     istrides[-2], #Rective field vertical stride bytes\n",
    "                                     istrides[-1]) #Receptive field horizontal stride bytes\n",
    "                            )\n",
    "        \n",
    "        intermediate_6D_transposed = intermediate_6D.transpose(0, 3, 1, 2, 4, 5)\n",
    "        intermediate_5D = intermediate_6D_transposed.reshape(batch_number, filters, pooled_h, pooled_w, pool_h * pool_w)\n",
    "\n",
    "        out_matrix = np.max(intermediate_5D, axis=-1)\n",
    "        IndexA_for5D = np.argmax(intermediate_5D, axis=-1)\n",
    "\n",
    "        def _backward():\n",
    "            # Recover window position (i, j) from flat index in last dim\n",
    "            Y.grad = np.zeros_like(Y.matrix) if Y.grad is None else Y.grad\n",
    "            flat_idx = IndexA_for5D  # (B, F, pooled_h, pooled_w)\n",
    "            i = flat_idx // pool_w\n",
    "            j = flat_idx % pool_w\n",
    "\n",
    "            # Build grids for batch, filter, and pooled positions\n",
    "            b_grid = np.arange(batch_number).reshape(batch_number, 1, 1, 1)\n",
    "            f_grid = np.arange(filters).reshape(1, filters, 1, 1)\n",
    "            ph_grid = np.arange(pooled_h).reshape(1, 1, pooled_h, 1)\n",
    "            pw_grid = np.arange(pooled_w).reshape(1, 1, 1, pooled_w)\n",
    "\n",
    "            # Broadcast all to shape (B, F, pooled_h, pooled_w)\n",
    "            b_idx = np.broadcast_to(b_grid, flat_idx.shape)\n",
    "            f_idx = np.broadcast_to(f_grid, flat_idx.shape)\n",
    "            ph = np.broadcast_to(ph_grid, flat_idx.shape)\n",
    "            pw = np.broadcast_to(pw_grid, flat_idx.shape)\n",
    "\n",
    "            # Compute actual positions in Y where max values came from\n",
    "            h_idx = self.stride * ph + i\n",
    "            w_idx = self.stride * pw + j\n",
    "\n",
    "            # Accumulate gradients using 4D indexing\n",
    "            np.add.at(Y.grad, (b_idx.ravel(), f_idx.ravel(), h_idx.ravel(), w_idx.ravel()), out.grad.ravel())\n",
    "\n",
    "        out = tensor(out_matrix, _children=(Y, ), _operation=\"maxpool\")\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bc4244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \n",
    "        self.bias = tensor.zeros((1, out_features))\n",
    "        self.weights = tensor.he_init((in_features, out_features), in_features)\n",
    "        #self.weights = tensor.random((out_features, in_features))\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights, self.bias]\n",
    "\n",
    "    def __call__(self, X:tensor):\n",
    "        return (X @ self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21f920ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, in_channels, layers, kernels_in_layers, kernels_shape, conv_strides, pool_shape, pool_strides, FCL_weights):\n",
    "\n",
    "        self.in_channels = (in_channels, ) + kernels_in_layers\n",
    "        self.FCL_weights = FCL_weights\n",
    "        self.layers = layers\n",
    "        #self.conv_layers = [Conv2d(in_channels[layer], kernels_in_layers[layer], kernels_shape[layer], conv_strides[layer]) for layer in range(layers)]\n",
    "        self.conv_layers = [Conv2d(self.in_channels[layer], kernels_in_layers[layer], kernels_shape[layer], conv_strides[layer]) for layer in range(layers)]\n",
    "        self.pool_layers = [maxpool2D(kernels_in_layers[layer], pool_shape[layer], pool_strides[layer]) for layer in range(layers)]\n",
    "        self.FC_layers = [None for _ in range(layers+1)]\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in range(self.layers):\n",
    "            params.extend(self.conv_layers[layer].parameters())\n",
    "            params.extend(self.FC_layers[layer].parameters())\n",
    "        #params.extend(self.FC_layers[layer+1].parameters())\n",
    "        return params\n",
    "\n",
    "    def __call__(self, X:tensor):\n",
    "        b = X\n",
    "        for layer in range(self.layers):\n",
    "            b = self.pool_layers[layer](self.conv_layers[layer](b).ReLU())\n",
    "        \n",
    "        out:tensor = b.reshape((X.shape[0], -1))\n",
    "        if self.FC_layers[0] is None:\n",
    "            self.FC_layers[0] = FC(out.shape[1], self.FCL_weights[0])\n",
    "\n",
    "        for layer in range(self.layers):\n",
    "            if self.FC_layers[layer] is None:\n",
    "                self.FC_layers[layer] = FC(self.FCL_weights[layer-1], self.FCL_weights[layer])\n",
    "\n",
    "            out = self.FC_layers[layer](out)\n",
    "            \n",
    "        \n",
    "        #out = c.transpose()\n",
    "        return out\n",
    "    \n",
    "    @classmethod\n",
    "    def cross_entropy_loss(cls, ypredicted: tensor, ytrue, batch_size):\n",
    "        ytrue =  tensor(ytrue) if not isinstance(ytrue, tensor) else ytrue\n",
    "        cross_entropy = -1*ypredicted.log_softmax(axis=-1)\n",
    "        loss = ((ytrue * cross_entropy).sum())/batch_size\n",
    "        #loss = ((ytrue * cross_entropy).sum())\n",
    "        return loss\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs = 10, lr = 0.001, batch_size = 32):\n",
    "        lossT = []\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            n_batches = 0\n",
    "            perm = np.random.permutation(len(X_train))\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                idx = perm[i:i+batch_size]\n",
    "                xb = tensor(X_train[idx])             \n",
    "                yb = tensor(y_train[idx]) \n",
    "\n",
    "                y_predicted = self(xb)\n",
    "                ce_loss = CNN.cross_entropy_loss(y_predicted, yb, len(idx))\n",
    "                ce_loss.backward()\n",
    "\n",
    "                for param in self.parameters():\n",
    "                    if param.grad is not None:\n",
    "\n",
    "                        #grad_clipped = np.clip(param.grad, -1.0, 1.0)\n",
    "                        #param.matrix -= lr * grad_clipped\n",
    "                        param.matrix -= lr * param.grad\n",
    "                        param.grad = None\n",
    "\n",
    "                epoch_loss += float(ce_loss.matrix.flatten()[0])\n",
    "                n_batches += 1\n",
    "\n",
    "                # Clean up\n",
    "                ce_loss.cleanBackward()\n",
    "                y_predicted.cleanBackward()\n",
    "                del xb, yb, y_predicted, ce_loss\n",
    "                if n_batches >= 50:\n",
    "                    gc.collect()\n",
    "            \n",
    "            avg_loss = epoch_loss / n_batches    \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "            lossT.append((epoch, avg_loss))\n",
    "\n",
    "        return lossT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f2f1a6",
   "metadata": {},
   "source": [
    "Now let's prepare the CIFAR-10 dataset.\n",
    "1. Load dataset\n",
    "2. Normalise training images\n",
    "3. Encode classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61f33850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pawanadhikari/Documents/Roadmap/MachineLearningMaths/.venv/lib/python3.14/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "/Users/pawanadhikari/Documents/Roadmap/MachineLearningMaths/.venv/lib/python3.14/site-packages/keras/src/datasets/cifar.py:18: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  d = cPickle.load(f, encoding=\"bytes\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'jax'\n",
    "from keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_train = X_train.transpose(0, 3, 1, 2).astype(np.float32) / 255.0  \n",
    "X_test = X_test.transpose(0, 3, 1, 2).astype(np.float32) / 255.0\n",
    "Y_train = np.zeros((y_train.size, 10))\n",
    "Y_train[np.arange(y_train.size), y_train.flatten()] = 1\n",
    "Y_test = np.zeros((y_test.size, 10))\n",
    "Y_test[np.arange(y_test.size), y_test.flatten()] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96cd0b5",
   "metadata": {},
   "source": [
    "Shapes for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e11f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b8ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing first 4 images.\n",
    "'''\n",
    "for image in X_test[:4]:\n",
    "    image = image.transpose(1,2,0)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "'''\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(4, 4))\n",
    "ax[0,0].imshow(X_test[0].transpose(1,2,0))\n",
    "ax[0,0].axis('off')\n",
    "ax[0,1].imshow(X_test[1].transpose(1,2,0))\n",
    "ax[0,1].axis('off')\n",
    "ax[1,0].imshow(X_test[2].transpose(1,2,0))\n",
    "ax[1,0].axis('off')\n",
    "ax[1,1].imshow(X_test[3].transpose(1,2,0))\n",
    "ax[1,1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddfd80e",
   "metadata": {},
   "source": [
    "And let's see the outputs now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a532f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef9d5e",
   "metadata": {},
   "source": [
    "After Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b554734",
   "metadata": {},
   "outputs": [],
   "source": [
    "del y_test, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa916e87",
   "metadata": {},
   "source": [
    "Now, Let us train a simple CNN for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a10b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 2.107396\n",
      "Epoch 2/20, Loss: 1.750964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored while calling GC callback <function _xla_gc_callback at 0x1104ee140>:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pawanadhikari/Documents/Roadmap/MachineLearningMaths/.venv/lib/python3.14/site-packages/jax/_src/lib/__init__.py\", line 124, in _xla_gc_callback\n",
      "    def _xla_gc_callback(*args):\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "model = CNN(\n",
    "    in_channels=3,\n",
    "    layers=2,\n",
    "    kernels_in_layers= (16, 32, ),\n",
    "    kernels_shape= (3, 3, ),\n",
    "    conv_strides= (1, 1, ),\n",
    "    pool_shape= (2, 2, ),\n",
    "    pool_strides= (2, 2, ), \n",
    "    FCL_weights= (128, 10) \n",
    ")\n",
    "\n",
    "#We will be training on 1/10th of the total dataset.\n",
    "loss = model.fit(X_train, Y_train, epochs=20, lr=0.06, batch_size=32)\n",
    "\n",
    "#Plot Train Loss vs Epochs graph.\n",
    "epochs, losses = zip(*loss)\n",
    "plt.plot(epochs, losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del  model, loss, epochs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "924d91b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del loss, epochs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee32dca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities (first 5 samples):\n",
      "[[7.6274350e-02 7.6274350e-02 7.6274350e-02 ... 7.6274350e-02\n",
      "  7.6274350e-02 7.6274350e-02]\n",
      " [4.0130269e-02 7.0109314e-01 4.9792073e-04 ... 4.9792073e-04\n",
      "  1.9283970e-01 6.2949352e-02]\n",
      " [1.7736168e-01 2.8668523e-01 3.3001050e-02 ... 3.3001050e-02\n",
      "  2.1166581e-01 5.4688636e-02]\n",
      " ...\n",
      " [2.9254308e-02 2.9254308e-02 2.9254308e-02 ... 7.0836313e-02\n",
      "  2.9254308e-02 2.9254308e-02]\n",
      " [7.0837303e-03 7.6228189e-01 6.6793729e-03 ... 5.2748360e-02\n",
      "  1.8193658e-03 1.3547671e-01]\n",
      " [7.8828965e-04 7.8828965e-04 1.0197742e-03 ... 7.7914453e-01\n",
      "  7.8828965e-04 7.8828965e-04]]\n",
      "Actual labels (first 5 samples):\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n",
      "Predicted classes: [5 1 1 0 6 6 1 6 2 9]\n",
      "True classes: [3 8 8 0 6 6 1 6 3 1]\n",
      "Accuracy: 51.3%\n"
     ]
    }
   ],
   "source": [
    "xtest = X_test\n",
    "ytest = Y_test\n",
    "\n",
    "\"\"\"\n",
    "for image in xtest:\n",
    "    plt.imshow(image.transpose(1,2,0))\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "ypred = model(tensor(xtest))\n",
    "probs = ypred.softmax()\n",
    "\n",
    "print(\"Predicted probabilities (first 5 samples):\")\n",
    "print(probs.matrix)\n",
    "print(\"Actual labels (first 5 samples):\")\n",
    "print(ytest)\n",
    "\n",
    "# Check argmax accuracy\n",
    "pred_classes = np.argmax(probs.matrix, axis=1)\n",
    "true_classes = np.argmax(ytest, axis=1)\n",
    "print(f\"Predicted classes: {pred_classes[:10]}\")\n",
    "print(f\"True classes: {true_classes[:10]}\")\n",
    "print(f\"Accuracy: {(pred_classes == true_classes).mean() * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b4a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
