{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f58126d",
   "metadata": {},
   "source": [
    "Apparently Log_softmax is causing NaN(Not a Number) error after multiple epochs. \n",
    "This is because we are doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c97fbbc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m out_matrix = \u001b[38;5;28;43mself\u001b[39;49m.matrix - np.log(np.sum(np.exp(\u001b[38;5;28mself\u001b[39m.matrix), axis=axis, keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[31mNameError\u001b[39m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "out_matrix = self.matrix - np.log(np.sum(np.exp(self.matrix), axis=axis, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e86cd",
   "metadata": {},
   "source": [
    "We see a crash at \"np.exp(self.matrix)\" because as the predictions start to become accurate, the logits are high and thus their exponentials become infinity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e29a15",
   "metadata": {},
   "source": [
    "thus we can apply a simple fix by subtracting down all the values in the matrix by the max term along their axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b1eb59",
   "metadata": {},
   "source": [
    "i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601db647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(self, axis=-1):\n",
    "        #Max value in the batch\n",
    "        c = np.max(self.matrix, axis=axis, keepdims=True)\n",
    "        \n",
    "        #Subtract max BEFORE exponentiating\n",
    "        log_sum_exp = c + np.log(np.sum(np.exp(self.matrix - c), axis=axis, keepdims=True))\n",
    "        \n",
    "        out_matrix = self.matrix - log_sum_exp\n",
    "        \n",
    "        return tensor(out_matrix) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146dd98c",
   "metadata": {},
   "source": [
    "To fix high memory usage, we have to redesign some parts. We have been simply using out.grad in our _backward() lamdas which cause circular referencing. Out own's the lamda _backward() which owns out. Thus, neither of them get cleared out. Thus, we need to decouple them by passing out.grad as an argument to _backward(). Yes! This does mean we have to change all our operations. But, this will be worth it. We will neither need Garbage Collector calls, nor weak referencing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
