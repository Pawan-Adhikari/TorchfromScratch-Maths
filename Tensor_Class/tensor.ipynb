{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce6cb4a",
   "metadata": {},
   "source": [
    "We have learned to build micrograds and autograds from scratch. We used a scalar class called \"Value\". However, as a Neural Network grows in size, it is extremely computationally inefficient to work with scalars only. Thus we need to implement a tensor class which holds matrices of nxm dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c1a62a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f45940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensor:\n",
    "    def __init__(self, fromArray=np.zeros((2,2)), _children = (), _operation = ''):\n",
    "        fromArray = fromArray if isinstance(fromArray, np.ndarray) else np.array(fromArray)\n",
    "        assert len(fromArray.shape) == 2, \"Only 2D Tensors or Scalar to 2D Supported!\"\n",
    "        self.matrix = fromArray\n",
    "        self.rows = fromArray.shape[0]\n",
    "        self.columns = fromArray.shape[1]\n",
    "        self._prev = set(_children)\n",
    "        self._operation = _operation\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor Values = {self.matrix}\"\n",
    "    \n",
    "    @classmethod\n",
    "    def zeros(cls, rows, columns, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = np.zeros((rows, columns), dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls, rows, columns, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.random.rand(rows, columns)).astype(dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def const(cls, rows, columns, constant=1, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.full((rows, columns), constant)).astype(dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    \n",
    "    def shape(self):\n",
    "        return (self.rows, self.columns)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix + other.matrix\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix - other.matrix\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = other.matrix - self.matrix\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix * other.matrix\n",
    "        out = tensor(out_matrix, (self, other), '*')\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "        \n",
    "    def __matmul__(self, other):\n",
    "        other = other if isinstance(other, tensor) else tensor(other)\n",
    "        assert other.shape()[0] == self.shape()[-1], \"Dimension Unsupported for @\"\n",
    "        out_matrix = self.matrix @ other.matrix\n",
    "        out = tensor(out_matrix, (self, other), '@')\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def transpose(self):\n",
    "        out_matrix = self.matrix.transpose()\n",
    "        out = tensor(out_matrix, (self, ), 'T')\n",
    "\n",
    "        return out\n",
    "\n",
    "    def checkOther(self, other):\n",
    "        if isinstance(other, int | float):\n",
    "            other = tensor.const(self.rows, self.columns, other)\n",
    "        elif not isinstance(other, tensor):\n",
    "            other = tensor(other)\n",
    "        assert other.shape() == self.shape(), \"Operand Tensor sizes dont match\"\n",
    "\n",
    "        return other\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d8b86b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor Values = [[3 2]\n",
       " [5 1]]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = [[3,5],\n",
    "        [2,1]]\n",
    "test2 = [[2,2],\n",
    "        [2,3]]\n",
    "t1 = tensor(test1)\n",
    "t2 = tensor(test2)\n",
    "\n",
    "t1 @ t2\n",
    "t1.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1cc4124f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Tensor Values = [[4 6]], Tensor Values = [[1 2]]}\n",
      "{Tensor Values = [[3 4]], Tensor Values = [[1 2]]}\n",
      "*\n",
      "+\n"
     ]
    }
   ],
   "source": [
    "a = tensor([[1, 2]])\n",
    "b = tensor([[3, 4]])\n",
    "c = a + b\n",
    "d = c * a\n",
    "\n",
    "print(d._prev)     \n",
    "print(c._prev)     \n",
    "print(d._operation) \n",
    "print(c._operation) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5375ae2",
   "metadata": {},
   "source": [
    "Lets save our current state of Tensor class as a checkpoint. Similar to Value class for micrograd and autograd, we have implemented operations and children tracking. We have successfully implemented the computation graph model for tensors. We can now move on to back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5133ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensor:\n",
    "    def __init__(self, fromArray=np.zeros((2,2)), _children = (), _operation = ''):\n",
    "        fromArray = fromArray if isinstance(fromArray, np.ndarray) else np.array(fromArray)\n",
    "        assert len(fromArray.shape) == 2, \"Only 2D Tensors or Scalar to 2D Supported!\"\n",
    "        self.matrix = fromArray\n",
    "        self.rows = fromArray.shape[0]\n",
    "        self.columns = fromArray.shape[1]\n",
    "        self._prev = set(_children)\n",
    "        self._operation = _operation\n",
    "        self.grad = None\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor Values = {self.matrix}\"\n",
    "    \n",
    "    @classmethod\n",
    "    def zeros(cls, rows, columns, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = np.zeros((rows, columns), dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls, rows, columns, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.random.rand(rows, columns)).astype(dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def const(cls, rows, columns, constant=1, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.full((rows, columns), constant)).astype(dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    \n",
    "    def shape(self):\n",
    "        return (self.rows, self.columns)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix + other.matrix\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix - other.matrix\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = other.matrix - self.matrix\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix * other.matrix\n",
    "        out = tensor(out_matrix, (self, other), '*')\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "        \n",
    "    def __matmul__(self, other):\n",
    "        other = other if isinstance(other, tensor) else tensor(other)\n",
    "        assert other.shape()[0] == self.shape()[-1], \"Dimension Unsupported for @\"\n",
    "        out_matrix = self.matrix @ other.matrix\n",
    "        out = tensor(out_matrix, (self, other), '@')\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def transpose(self):\n",
    "        out_matrix = self.matrix.transpose()\n",
    "        out = tensor(out_matrix, (self, ), 'T')\n",
    "\n",
    "        return out\n",
    "\n",
    "    def checkOther(self, other):\n",
    "        if isinstance(other, int | float):\n",
    "            other = tensor.const(self.rows, self.columns, other)\n",
    "        elif not isinstance(other, tensor):\n",
    "            other = tensor(other)\n",
    "        assert other.shape() == self.shape(), \"Operand Tensor sizes dont match\"\n",
    "\n",
    "        return other\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1d24bcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "t = tensor()\n",
    "print(t.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3946c48a",
   "metadata": {},
   "source": [
    "Now let us add the backward lamda functions for each operation. This would be a headache. Either you can first derive them on paper yourself based on the concepts of Jacobians, downstream/upstream/local gradients, tensor contaction etc. or use standard formula. The first one is recommended for sound understanding. For reference, there are notes present in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e0f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensor:\n",
    "    def __init__(self, fromArray=np.zeros((2,2)), _children = (), _operation = ''):\n",
    "        fromArray = fromArray if isinstance(fromArray, np.ndarray) else np.array(fromArray)\n",
    "        assert len(fromArray.shape) == 2, \"Only 2D Tensors or Scalar to 2D Supported!\"\n",
    "        self.matrix = fromArray\n",
    "        self.rows = fromArray.shape[0]\n",
    "        self.columns = fromArray.shape[1]\n",
    "        self._prev = set(_children)\n",
    "        self._operation = _operation\n",
    "        self._backward = lambda : None\n",
    "        self.grad = None\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor Values = {self.matrix}\"\n",
    "    \n",
    "    @classmethod\n",
    "    def zeros(cls, rows, columns, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = np.zeros((rows, columns), dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls, rows, columns, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.random.rand(rows, columns)).astype(dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def const(cls, rows, columns, constant=1, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.full((rows, columns), constant)).astype(dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    \n",
    "    def shape(self):\n",
    "        return (self.rows, self.columns)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix + other.matrix\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(out.grad) if other.grad is None else other.grad\n",
    "            self.grad += out.grad #Derivation in the notes. \n",
    "            other.grad += out.grad\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return self + (-1 * other)\n",
    "    \n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return other + (-1 * other)\n",
    "    \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix * other.matrix\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(out.grad) if other.grad is None else other.grad\n",
    "            self.grad += out.grad * other.matrix #Derivation in the notes. \n",
    "            other.grad += out.grad * self.matrix\n",
    "\n",
    "        out = tensor(out_matrix, (self, other), '*')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "        \n",
    "    def __matmul__(self, other):\n",
    "        other = other if isinstance(other, tensor) else tensor(other)\n",
    "        assert other.shape()[0] == self.shape()[-1], \"Dimension Unsupported for @\"\n",
    "        out_matrix = self.matrix @ other.matrix\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(other.matrix) if other.grad is None else other.grad\n",
    "            self.grad += out.grad @ (other.matrix).transpose()#Derivation in the notes.\n",
    "            other.grad += (self.matrix).transpose() @ out.grad \n",
    "        out = tensor(out_matrix, (self, other), '@')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def transpose(self):\n",
    "        out_matrix = self.matrix.transpose()\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad.transpose()) if self.grad is None else self.grad\n",
    "            self.grad += (out.grad).transpose() #Not in note, but can be derived similarly.\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), 'T')\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, N):\n",
    "        assert isinstance(N, int | float), \"Can only power up by scalars!\"\n",
    "        out_matrix = self.matrix ** N\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += N * (self.matrix ** (N-1)) * out.grad\n",
    "        \n",
    "        out = tensor(out_matrix, _children=(self, ), _operation=\"**\")\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __div__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return self * (other**-1)\n",
    "    \n",
    "    def __rdiv__(self, other):\n",
    "        return other * (self**-1)\n",
    "    \n",
    "    def sum(self):\n",
    "        out_matrix = np.array(([[self.matrix.sum()]]))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += np.ones_like(self.matrix) * out.grad\n",
    "\n",
    "        out = tensor(out_matrix, _children=(self, ), _operation='sum()')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def mean(self):\n",
    "        N = self.rows * self.columns\n",
    "        out_matrix = np.array(([[self.matrix.sum()/(N)]]))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += np.ones_like(self.matrix) * out.grad / N\n",
    "\n",
    "        out = tensor(out_matrix, _children=(self, ), _operation='mean()')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def checkOther(self, other):\n",
    "        if isinstance(other, int | float):\n",
    "            other = tensor.const(self.rows, self.columns, other)\n",
    "        elif not isinstance(other, tensor):\n",
    "            other = tensor(other)\n",
    "        assert other.shape() == self.shape(), \"Operand Tensor sizes dont match\"\n",
    "\n",
    "        return other\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad = None\n",
    "        \n",
    "    def backward(self):\n",
    "        self.grad = np.ones_like(self.matrix)\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        for current in reversed(topo):\n",
    "\n",
    "            current._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae85391f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5. 10.]]\n"
     ]
    }
   ],
   "source": [
    "a = tensor([[2.0, 3.0]])\n",
    "b = tensor([[1.0, 4.0]])\n",
    "c = a + b\n",
    "d = c * a\n",
    "d.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac94d43b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
