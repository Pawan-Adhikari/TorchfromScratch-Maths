{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "969b9f96",
   "metadata": {},
   "source": [
    "The goal of this notebook is to visualise individual activation maps and pools from various layer to get a sense of what's happening inside our model. We are essentially trying to white box the model. This is sort of like \"https://adamharley.com/nn_vis/cnn/2d.html\" but for CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b398a1",
   "metadata": {},
   "source": [
    "Firstly the imports!\n",
    "- The entire model runs in numpy python (CPU), but uses vectorised, efficient im2col and pooling operations. \n",
    "- itertools is for specialised iteration for certain operation of tensor class\n",
    "- matplotlib for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5646f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550237f",
   "metadata": {},
   "source": [
    "This is the entire backbone of our model. \n",
    "- Firstly, we have the tensor class. It is a wrapper on numpy array that performs vector operations with child tracking and computational graph.\n",
    "- Secondly the Conv2d is the 2D convolutional layer with im2col operation. \n",
    "- The maxpool2D class is simply the pooling layer class.\n",
    "- FC (Fully Connected) class is the backbone of fully connected layer.\n",
    "\n",
    "Note: We dont have batch normalization yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae29521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensor:\n",
    "    def __init__(self, fromArray=np.zeros((2,2)), _children = (), _operation = ''):\n",
    "        fromArray = fromArray if isinstance(fromArray, np.ndarray) else np.array(fromArray)\n",
    "        #assert len(fromArray.shape) == 2, \"Only 2D Tensors or Scalar to 2D Supported!\"\n",
    "        self.matrix = fromArray\n",
    "        #self.rows = fromArray.shape[0]\n",
    "        #self.columns = fromArray.shape[1]\n",
    "        self.shape = fromArray.shape\n",
    "        self._prev = set(_children)\n",
    "        self._operation = _operation\n",
    "        self._backward = lambda : None\n",
    "        self.grad = None\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor Values = {self.matrix}\"\n",
    "    \n",
    "    @classmethod\n",
    "    def zeros(cls, shape, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = np.zeros(shape, dtype=dtype)\n",
    "        t.shape = shape\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls, shape, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.random.randn(*shape) * 0.1).astype(dtype=dtype)\n",
    "        t.shape = shape\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def he_init(cls, shape, fan_in, dtype=np.float32):\n",
    "        t = tensor()\n",
    "        std = np.sqrt(2.0 / fan_in)\n",
    "        t.matrix = (np.random.randn(*shape) * std).astype(dtype=dtype)\n",
    "        t.shape = shape\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def const(cls, shape, constant=1, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.full(shape, constant)).astype(dtype=dtype)\n",
    "        t.shape = shape\n",
    "        return t\n",
    "    \n",
    "    #Operations\n",
    "    def __add__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix + other.matrix\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(other.matrix) if other.grad is None else other.grad\n",
    "            out1 = self.return_unbroadcasted(out)\n",
    "            out2 = other.return_unbroadcasted(out)\n",
    "            self.grad += out1 #Derivation in the notes. \n",
    "            other.grad += out2\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return self + (-1 * other)\n",
    "    \n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return other + (-1 * other)\n",
    "    \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix * other.matrix\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(out.grad) if other.grad is None else other.grad\n",
    "            out1 = self.return_unbroadcasted(out)\n",
    "            out2 = other.return_unbroadcasted(out)\n",
    "            self.grad += out1* other.matrix #Derivation in the notes. \n",
    "            other.grad += out2 * self.matrix\n",
    "\n",
    "        out = tensor(out_matrix, (self, other), '*')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return self*other\n",
    "    \n",
    "    '''\n",
    "    batch multiplication might cause shape broadcasts.\n",
    "    eg. (3,2,2) @ (1,2,3) = (3,2,3)\n",
    "    this is similar to our element wise operations\n",
    "    thus we should be handling this the same way we did for elementwise operations\n",
    "    But, for now, we would be working in a controlled way (Even for CNNS)\n",
    "    and wouldn't need this handling.\n",
    "    '''\n",
    "    def __matmul__(self, other):\n",
    "        other = other if isinstance(other, tensor) else tensor(other)\n",
    "        assert other.shape[-2] == self.shape[-1], \"Dimension Unsupported for @\"\n",
    "        out_matrix = self.matrix @ other.matrix\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(other.matrix) if other.grad is None else other.grad\n",
    "            self.grad += out.grad @ (other.matrix).swapaxes(-2,-1)#Derivation in the notes.\n",
    "            other.grad += (self.matrix).swapaxes(-2,-1) @ out.grad \n",
    "        out = tensor(out_matrix, (self, other), '@')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "\n",
    "    #I and thus we should learn at this point that to make our class compatible for ND tensors,\n",
    "    #We need the matrix multiplication and Transpose backward to change\n",
    "    #For higher dimensions, matmul = batch matmul where multiplication is done \n",
    "    #along each and every batches of 2D matrix. \n",
    "    #eg. If we have (2,3,3) shape tensor, it implies there are two batches of (3,3) matrices\n",
    "    #similarly, (2,3,3,2) shape = 2x3 batches of 3x2 matrices.\n",
    "    #matrix multiplication, (2,3,3) @ (2,3,2) = (2,3,2)\n",
    "    def swap_axes(self, axis1, axis2):\n",
    "        out_matrix = self.matrix.swapaxes(axis1, axis2)\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad.swapaxes(axis1,axis2)) if self.grad is None else self.grad\n",
    "            self.grad += (out.grad).swapaxes(axis1,axis2) #Not in note, but can be derived similarly.\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), 'T')\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def transpose(self):\n",
    "        out_matrix = self.matrix.transpose()\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad.transpose()) if self.grad is None else self.grad\n",
    "            self.grad += (out.grad).transpose() #Not in note, but can be derived similarly.\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), 'T')\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __rmatmul__(self, other):\n",
    "        other = other if isinstance(other, tensor) else tensor(other)\n",
    "        return other @ self\n",
    "    \n",
    "    def __pow__(self, N):\n",
    "        assert isinstance(N, int | float), \"Can only power up by scalars!\"\n",
    "        out_matrix = self.matrix ** N\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            out1 = self.return_unbroadcasted(out)\n",
    "            self.grad += N * (self.matrix ** (N-1)) * out1\n",
    "        \n",
    "        out = tensor(out_matrix, _children=(self, ), _operation=\"**\")\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return self * (other**-1)\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return other * (self**-1)\n",
    "    \n",
    "    def sum(self):\n",
    "        out_matrix = np.array(([[self.matrix.sum()]]))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += np.ones_like(self.matrix) * out.grad\n",
    "\n",
    "        out = tensor(out_matrix, _children=(self, ), _operation='sum()')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def mean(self):\n",
    "        N = np.prod(self.shape)\n",
    "        out_matrix = np.array(([[self.matrix.sum()/(N)]]))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += np.ones_like(self.matrix) * out.grad / N\n",
    "\n",
    "        out = tensor(out_matrix, _children=(self, ), _operation='mean()')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def ReLU(self):\n",
    "        out_matrix = np.maximum(0,self.matrix)\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += (self.matrix > 0).astype(self.matrix.dtype) * out.grad\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), \"ReLU\")\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def reshape(self, shape):\n",
    "        assert isinstance(shape, tuple), f\"Can only reshape using shape tuples e.g. (3,3). Provided is {shape}\"\n",
    "        out_matrix = self.matrix.reshape(shape)\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += out.grad.reshape(self.shape)\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), \"reshape()\")\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def flatten(self):\n",
    "        out_matrix = self.matrix.reshape(-1,np.prod(self.shape[1:]))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += out.grad.reshape(self.shape)\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), \"flatten()\")\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    #Helper Functions\n",
    "    #def shape(self):\n",
    "     #   return (self.rows, self.columns)\n",
    "\n",
    "    def return_unbroadcasted(self, out):  \n",
    "        added_axis = []\n",
    "        stretched_axis = []\n",
    "        for index, (first_no, second_no) in enumerate(itertools.zip_longest(reversed(self.shape), reversed(out.shape))):\n",
    "            if first_no is None:\n",
    "                added_axis.append(index)\n",
    "            elif (first_no == 1) and (second_no > 1):\n",
    "                stretched_axis.append(index)\n",
    "        grad = out.grad\n",
    "        ndim = len(out.shape)\n",
    "        if stretched_axis:\n",
    "            original_axes = tuple(ndim - 1 - i for i in stretched_axis)\n",
    "            grad = np.sum(grad, axis=original_axes, keepdims=True)\n",
    "        if added_axis:\n",
    "            original_axes = tuple(ndim - 1 - i for i in added_axis)\n",
    "            grad = np.sum(grad, axis=original_axes, keepdims=False)\n",
    "        return grad\n",
    "\n",
    "    def checkOther(self, other):\n",
    "        if isinstance(other, int | float):\n",
    "            other = tensor.const(self.shape, other)\n",
    "        elif not isinstance(other, tensor):\n",
    "            other = tensor(other)\n",
    "        #assert other.shape == self.shape, \"Operand Tensor sizes dont match\"\n",
    "\n",
    "        return other\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad = None\n",
    "        \n",
    "    def backward(self):\n",
    "        self.grad = np.ones_like(self.matrix, dtype=float)\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        for current in reversed(topo):\n",
    "\n",
    "            current._backward()\n",
    "\n",
    "    def exp(self):\n",
    "        out_matrix = np.exp(self.matrix)\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += out_matrix * out.grad  \n",
    "        \n",
    "        out = tensor(out_matrix, (self,), 'exp')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def log(self, eps=1e-8):\n",
    "        clipped = np.clip(self.matrix, eps, None)  \n",
    "        out_matrix = np.log(clipped)\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += (1.0 / clipped) * out.grad \n",
    "        \n",
    "        out = tensor(out_matrix, (self,), 'log')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def softmax(self, axis=-1):\n",
    "        out_matrix = np.exp(self.matrix) / np.sum(np.exp(self.matrix), axis = axis, keepdims=True)\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += out_matrix*(out.grad - np.sum(out_matrix * out.grad, axis = axis, keepdims=True))\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), 'softmax')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    __array_ufunc__ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f573ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d:\n",
    "    def __init__(self, in_channels=1, out_channels=1, kernel_size=2, stride=1):\n",
    "        self.kernel = tensor.random((out_channels, in_channels, kernel_size, kernel_size))\n",
    "        #fan_in = in_channels * kernel_size * kernel_size\n",
    "        #self.kernel = tensor.he_init((out_channels, in_channels, kernel_size, kernel_size), fan_in)\n",
    "        self.bias = tensor.zeros((out_channels, ))\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        self.in_channels = in_channels\n",
    "        self.stride = stride\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.kernel, self.bias]\n",
    "\n",
    "    def __call__(self, X : tensor):\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "\n",
    "        X_col, act_h, act_w = Conv2d.im2col(X, kernel_size=self.kernel_size, stride=self.stride)\n",
    "        K_col_shape = (self.out_channels, self.kernel_size*self.kernel_size*self.in_channels)\n",
    "        K_col = self.kernel.reshape(K_col_shape).transpose()\n",
    "        Y_col = X_col @ K_col + self.bias\n",
    "        Y = Y_col.reshape((batch_size, self.out_channels, act_h, act_w))\n",
    "        return Y\n",
    "        \n",
    "    @classmethod\n",
    "    def im2col(cls, X : tensor, kernel_size=2, stride=1):\n",
    "\n",
    "        batch_size = X.shape[0]\n",
    "        channels = X.shape[1]\n",
    "        image_height = X.shape[-2] #Rows\n",
    "        image_width = X.shape[-1] #Columns\n",
    "\n",
    "        #We are assuming square kernels.\n",
    "        kernel_h = kernel_size\n",
    "        kernel_w = kernel_size\n",
    "\n",
    "        act_h = (((image_height - kernel_size)//stride) + 1) #height of activation\n",
    "        act_w = (((image_width - kernel_size)//stride) + 1)  #width of activation\n",
    "\n",
    "        istrides = X.matrix.strides #strides of input tensor\n",
    "\n",
    "        intermediate_6D = np.lib.stride_tricks.as_strided(\n",
    "                            X.matrix,\n",
    "                            shape=(batch_size, act_h, act_w, channels, kernel_h, kernel_w),\n",
    "                            strides=(istrides[0], #No of images stride bytes\n",
    "                                     istrides[-2] * stride, #Activation map Vertical stride bytes\n",
    "                                     istrides[-1] * stride, #Activation map Horizontal stride bytes\n",
    "                                     istrides[1], #Channel stride bytes\n",
    "                                     istrides[-2], #Rective field vertical stride bytes\n",
    "                                     istrides[-1]) #Receptive field horizontal stride bytes\n",
    "                            )\n",
    "        \n",
    "        out_shape = (batch_size * act_h * act_w, channels * kernel_h * kernel_w)\n",
    "        out_matrix = np.reshape(intermediate_6D, shape=out_shape)\n",
    "\n",
    "\n",
    "        def _backward():\n",
    "            X.grad = np.zeros_like(X.matrix) if X.grad is None else X.grad\n",
    "            \n",
    "            grad_6D = out.grad.reshape(batch_size, act_h, act_w, channels, kernel_h, kernel_w)\n",
    "            for i in range(kernel_h):\n",
    "                for j in range(kernel_w):\n",
    "                    grad_slice = grad_6D[:, :, :, :, i, j]\n",
    "                    \n",
    "                    grad_slice_transposed = grad_slice.transpose(0, 3, 1, 2)\n",
    "                    X.grad[:, :, \n",
    "                        i : i + act_h * stride : stride, \n",
    "                        j : j + act_w * stride : stride\n",
    "                    ] += grad_slice_transposed\n",
    "\n",
    "        out = tensor(out_matrix, _children=(X, ), _operation='im2col')\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out, act_h, act_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49fb6676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class maxpool2D:\n",
    "    def __init__(self, in_channels, pool_size = 2, stride = 1):\n",
    "        self.in_channels = in_channels\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "    def __call__(self, Y: tensor):\n",
    "\n",
    "        batch_number = Y.shape[0]\n",
    "        filters = Y.shape[1]\n",
    "        image_height = Y.shape[-2] #Rows\n",
    "        image_width = Y.shape[-1] #Columns\n",
    "\n",
    "\n",
    "        #We are assuming square kernels.\n",
    "        pool_h = self.pool_size\n",
    "        pool_w = self.pool_size\n",
    "\n",
    "        pooled_h = (((image_height - pool_h)//self.stride) + 1) #height of activation\n",
    "        pooled_w = (((image_width - pool_w)//self.stride) + 1)  #width of activation\n",
    "\n",
    "        istrides = Y.matrix.strides #strides of input tensor\n",
    "\n",
    "        intermediate_6D = np.lib.stride_tricks.as_strided(\n",
    "                            Y.matrix,\n",
    "                            shape=(batch_number, pooled_h, pooled_w, filters, pool_h, pool_w),\n",
    "                            strides=(istrides[0], #No of images stride bytes\n",
    "                                     istrides[-2] * self.stride, #Activation map Vertical stride bytes\n",
    "                                     istrides[-1] * self.stride, #Activation map Horizontal stride bytes\n",
    "                                     istrides[1], #Channel stride bytes\n",
    "                                     istrides[-2], #Rective field vertical stride bytes\n",
    "                                     istrides[-1]) #Receptive field horizontal stride bytes\n",
    "                            )\n",
    "        \n",
    "        intermediate_6D_transposed = intermediate_6D.transpose(0, 3, 1, 2, 4, 5)\n",
    "        intermediate_5D = intermediate_6D_transposed.reshape(batch_number, filters, pooled_h, pooled_w, pool_h * pool_w)\n",
    "\n",
    "        out_matrix = np.max(intermediate_5D, axis=-1)\n",
    "        IndexA_for5D = np.argmax(intermediate_5D, axis=-1)\n",
    "\n",
    "        def _backward():\n",
    "            # Recover window position (i, j) from flat index in last dim\n",
    "            Y.grad = np.zeros_like(Y.matrix) if Y.grad is None else Y.grad\n",
    "            flat_idx = IndexA_for5D  # (B, F, pooled_h, pooled_w)\n",
    "            i = flat_idx // pool_w\n",
    "            j = flat_idx % pool_w\n",
    "\n",
    "            # Build grids for batch, filter, and pooled positions\n",
    "            b_grid = np.arange(batch_number).reshape(batch_number, 1, 1, 1)\n",
    "            f_grid = np.arange(filters).reshape(1, filters, 1, 1)\n",
    "            ph_grid = np.arange(pooled_h).reshape(1, 1, pooled_h, 1)\n",
    "            pw_grid = np.arange(pooled_w).reshape(1, 1, 1, pooled_w)\n",
    "\n",
    "            # Broadcast all to shape (B, F, pooled_h, pooled_w)\n",
    "            b_idx = np.broadcast_to(b_grid, flat_idx.shape)\n",
    "            f_idx = np.broadcast_to(f_grid, flat_idx.shape)\n",
    "            ph = np.broadcast_to(ph_grid, flat_idx.shape)\n",
    "            pw = np.broadcast_to(pw_grid, flat_idx.shape)\n",
    "\n",
    "            # Compute actual positions in Y where max values came from\n",
    "            h_idx = self.stride * ph + i\n",
    "            w_idx = self.stride * pw + j\n",
    "\n",
    "            # Accumulate gradients using 4D indexing\n",
    "            np.add.at(Y.grad, (b_idx.ravel(), f_idx.ravel(), h_idx.ravel(), w_idx.ravel()), out.grad.ravel())\n",
    "\n",
    "        out = tensor(out_matrix, _children=(Y, ), _operation=\"maxpool\")\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bc4244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \n",
    "        self.bias = tensor.zeros((out_features, 1))\n",
    "        # He initialization: fan_in = in_features\n",
    "        #self.weights = tensor.he_init((out_features, in_features), in_features)\n",
    "        self.weights = tensor.random((out_features, in_features))\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights, self.bias]\n",
    "\n",
    "    def __call__(self, X:tensor):\n",
    "        return (self.weights @ X) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21f920ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, in_channels, layers, kernels_in_layers, kernels_shape, conv_strides, pool_shape, pool_strides, FCL_weights):\n",
    "        self.in_channels = (in_channels, ) + kernels_in_layers\n",
    "        self.FCL_weights = FCL_weights\n",
    "        self.layers = layers\n",
    "        self.conv_layers = [Conv2d(self.in_channels[layer], kernels_in_layers[layer], kernels_shape[layer], conv_strides[layer]) for layer in range(layers)]\n",
    "        self.pool_layers = [maxpool2D(kernels_in_layers[layer], pool_shape[layer], pool_strides[layer]) for layer in range(layers)]\n",
    "        self.FC_layers = [None for _ in range(layers+1)]\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in range(self.layers):\n",
    "            params.extend(self.conv_layers[layer].parameters())\n",
    "            params.extend(self.FC_layers[layer].parameters())\n",
    "        params.extend(self.FC_layers[layer+1].parameters())\n",
    "        return params\n",
    "\n",
    "    def __call__(self, X:tensor):\n",
    "        b = X\n",
    "        for layer in range(self.layers):\n",
    "            a_ = self.conv_layers[layer](b)\n",
    "            a = a_.ReLU()\n",
    "            b = self.pool_layers[layer](a)\n",
    "        \n",
    "        c:tensor = b.reshape((X.shape[0], -1)).transpose()\n",
    "        if self.FC_layers[0] is None:\n",
    "            self.FC_layers[0] = FC(c.shape[0], self.FCL_weights[0])\n",
    "\n",
    "        for layer in range(self.layers+1):\n",
    "            if self.FC_layers[layer] is None:\n",
    "                self.FC_layers[layer] = FC(self.FCL_weights[layer-1], self.FCL_weights[layer])\n",
    "\n",
    "            c = self.FC_layers[layer](c)\n",
    "            \n",
    "            if layer < self.layers:\n",
    "                c = c.ReLU()\n",
    "        \n",
    "        out = c.transpose()\n",
    "        return out\n",
    "    \n",
    "    @classmethod\n",
    "    def cross_entropy_loss(cls, ypredicted: tensor, ytrue, batch_size):\n",
    "        ytrue =  tensor(ytrue) if not isinstance(ytrue, tensor) else ytrue\n",
    "        ypredicted = ypredicted.softmax(axis=-1)\n",
    "        cross_entropy = -1 * ypredicted.log()\n",
    "        #loss = ((ytrue * cross_entropy).sum())/batch_size\n",
    "        loss = ((ytrue * cross_entropy).sum())\n",
    "        return loss\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs = 10, lr = 0.001, batch_size = 32):\n",
    "        lossT = []\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            n_batches = 0\n",
    "            perm = np.random.permutation(len(X_train))\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                idx = perm[i:i+batch_size]\n",
    "                xb = tensor(X_train[idx])             \n",
    "                yb = tensor(y_train[idx]) \n",
    "\n",
    "                y_predicted = self(xb)\n",
    "                ce_loss = CNN.cross_entropy_loss(y_predicted, yb, len(idx))\n",
    "                ce_loss.backward()\n",
    "\n",
    "                for param in self.parameters():\n",
    "                    if param.grad is not None:\n",
    "\n",
    "                        grad_clipped = np.clip(param.grad, -1.0, 1.0)\n",
    "                        param.matrix -= lr * grad_clipped\n",
    "                        param.grad = None\n",
    "                \n",
    "                epoch_loss += ce_loss.matrix.flatten()[0]\n",
    "                n_batches += 1\n",
    "            \n",
    "            avg_loss = epoch_loss / n_batches    \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "            lossT.append((epoch, avg_loss))\n",
    "\n",
    "        return lossT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f2f1a6",
   "metadata": {},
   "source": [
    "Now let's prepare the CIFAR-10 dataset.\n",
    "1. Load dataset\n",
    "2. Normalise training images\n",
    "3. Encode classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61f33850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pawanadhikari/Documents/Roadmap/MachineLearningMaths/.venv/lib/python3.14/site-packages/keras/src/datasets/cifar.py:18: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  d = cPickle.load(f, encoding=\"bytes\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'jax'\n",
    "from keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "X_train = X_train.transpose(0, 3, 1, 2).astype(np.float32) / 255.0  \n",
    "X_test = X_test.transpose(0, 3, 1, 2).astype(np.float32) / 255.0\n",
    "Y_train = np.zeros((y_train.size, 10))\n",
    "Y_train[np.arange(y_train.size), y_train.flatten()] = 1\n",
    "Y_test = np.zeros((y_test.size, 10))\n",
    "Y_test[np.arange(y_test.size), y_test.flatten()] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96cd0b5",
   "metadata": {},
   "source": [
    "Shapes for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66e11f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 1, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5b8ecf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Showing first 4 images.\n",
    "'''\n",
    "for image in X_test[:4]:\n",
    "    image = image.transpose(1,2,0)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "'''\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(4, 4))\n",
    "ax[0,0].imshow(X_test[0].transpose(1,2,0))\n",
    "ax[0,0].axis('off')\n",
    "ax[0,1].imshow(X_test[1].transpose(1,2,0))\n",
    "ax[0,1].axis('off')\n",
    "ax[1,0].imshow(X_test[2].transpose(1,2,0))\n",
    "ax[1,0].axis('off')\n",
    "ax[1,1].imshow(X_test[3].transpose(1,2,0))\n",
    "ax[1,1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddfd80e",
   "metadata": {},
   "source": [
    "And let's see the outputs now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57a532f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3],\n",
       "       [8],\n",
       "       [8],\n",
       "       [0],\n",
       "       [6],\n",
       "       [6],\n",
       "       [1],\n",
       "       [6],\n",
       "       [3],\n",
       "       [1]], dtype=uint8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ef9d5e",
   "metadata": {},
   "source": [
    "After Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "092d732b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa916e87",
   "metadata": {},
   "source": [
    "Now, Let us train a simple CNN for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a10b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 73.366431\n",
      "Epoch 2/50, Loss: 73.368866\n",
      "Epoch 3/50, Loss: 73.359033\n",
      "Epoch 4/50, Loss: 73.375842\n",
      "Epoch 5/50, Loss: 73.369964\n",
      "Epoch 6/50, Loss: 73.367862\n",
      "Epoch 7/50, Loss: 73.358166\n",
      "Epoch 8/50, Loss: 73.368623\n",
      "Epoch 9/50, Loss: 73.370668\n"
     ]
    }
   ],
   "source": [
    "model = CNN(\n",
    "    in_channels=3,\n",
    "    layers=3,\n",
    "    kernels_in_layers= (5, 16, 32, ),\n",
    "    kernels_shape= (5, 5, 5, ),\n",
    "    conv_strides= (1, 1, 1, ),\n",
    "    pool_shape= (2, 2, 2, ),\n",
    "    pool_strides= (2, 2, 2, ), \n",
    "    FCL_weights= (128, 64, 32, 10) #4 Fully Connected Layers including one output layer. \n",
    ")\n",
    "\n",
    "#We will be training on 1/10th of the total dataset.\n",
    "loss = model.fit(X_train[:5000], Y_train[:5000], epochs=50, lr=0.01, batch_size=32)\n",
    "\n",
    "#Plot Train Loss vs Epochs graph.\n",
    "epochs, losses = zip(*loss)\n",
    "plt.plot(epochs, losses)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee32dca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
