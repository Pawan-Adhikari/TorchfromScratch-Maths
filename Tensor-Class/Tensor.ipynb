{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce6cb4a",
   "metadata": {},
   "source": [
    "We have learned to build micrograds and autograds from scratch. We used a scalar class called \"Value\". However, as a Neural Network grows in size, it is extremely computationally inefficient to work with scalars only. Thus we need to implement a tensor class which holds matrices of nxm dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c1a62a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f45940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensor:\n",
    "    def __init__(self, fromArray=np.zeros((2,2)), _children = (), _operation = ''):\n",
    "        fromArray = fromArray if isinstance(fromArray, np.ndarray) else np.array(fromArray)\n",
    "        assert len(fromArray.shape) == 2, \"Only 2D Tensors or Scalar to 2D Supported!\"\n",
    "        self.matrix = fromArray\n",
    "        self.rows = fromArray.shape[0]\n",
    "        self.columns = fromArray.shape[1]\n",
    "        self._prev = set(_children)\n",
    "        self._operation = _operation\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor = {self.matrix}\"\n",
    "    \n",
    "    @classmethod\n",
    "    def zeros(cls, rows, columns, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = np.zeros((rows, columns), dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls, rows, columns, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.random.rand(rows, columns)).astype(dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def const(cls, rows, columns, constant=1, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.full((rows, columns), constant)).astype(dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    \n",
    "    def shape(self):\n",
    "        return (self.rows, self.columns)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix + other.matrix\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix - other.matrix\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = other.matrix - self.matrix\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix * other.matrix\n",
    "        out = tensor(out_matrix, (self, other), '*')\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "        \n",
    "    def __matmul__(self, other):\n",
    "        other = other if isinstance(other, tensor) else tensor(other)\n",
    "        assert other.shape()[0] == self.shape()[-1], \"Dimension Unsupported for @\"\n",
    "        out_matrix = self.matrix @ other.matrix\n",
    "        out = tensor(out_matrix, (self, other), '@')\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def transpose(self):\n",
    "        out_matrix = self.matrix.transpose()\n",
    "        out = tensor(out_matrix, (self, ), 'T')\n",
    "\n",
    "        return out\n",
    "\n",
    "    def checkOther(self, other):\n",
    "        if isinstance(other, int | float):\n",
    "            other = tensor.const(self.rows, self.columns, other)\n",
    "        elif not isinstance(other, tensor):\n",
    "            other = tensor(other)\n",
    "        assert other.shape() == self.shape(), \"Operand Tensor sizes dont match\"\n",
    "\n",
    "        return other\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8b86b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor = [[3 2]\n",
       " [5 1]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = [[3,5],\n",
    "        [2,1]]\n",
    "test2 = [[2,2],\n",
    "        [2,3]]\n",
    "t1 = tensor(test1)\n",
    "t2 = tensor(test2)\n",
    "\n",
    "t1 @ t2\n",
    "t1.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1cc4124f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Tensor = [[4 6]], Tensor = [[1 2]]}\n",
      "{Tensor = [[3 4]], Tensor = [[1 2]]}\n",
      "*\n",
      "+\n"
     ]
    }
   ],
   "source": [
    "a = tensor([[1, 2]])\n",
    "b = tensor([[3, 4]])\n",
    "c = a + b\n",
    "d = c * a\n",
    "\n",
    "print(d._prev)     \n",
    "print(c._prev)     \n",
    "print(d._operation) \n",
    "print(c._operation) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5375ae2",
   "metadata": {},
   "source": [
    "Lets save our current state of Tensor class as a checkpoint. Similar to Value class for micrograd and autograd, we have implemented operations and children tracking. We have successfully implemented the computation graph model for tensors. We can now move on to back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5133ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensor:\n",
    "    def __init__(self, fromArray=np.zeros((2,2)), _children = (), _operation = ''):\n",
    "        fromArray = fromArray if isinstance(fromArray, np.ndarray) else np.array(fromArray)\n",
    "        assert len(fromArray.shape) == 2, \"Only 2D Tensors or Scalar to 2D Supported!\"\n",
    "        self.matrix = fromArray\n",
    "        self.rows = fromArray.shape[0]\n",
    "        self.columns = fromArray.shape[1]\n",
    "        self._prev = set(_children)\n",
    "        self._operation = _operation\n",
    "        self.grad = None\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor Values = {self.matrix}\"\n",
    "    \n",
    "    @classmethod\n",
    "    def zeros(cls, rows, columns, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = np.zeros((rows, columns), dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls, rows, columns, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.random.rand(rows, columns)).astype(dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def const(cls, rows, columns, constant=1, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.full((rows, columns), constant)).astype(dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    \n",
    "    def shape(self):\n",
    "        return (self.rows, self.columns)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix + other.matrix\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix - other.matrix\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = other.matrix - self.matrix\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix * other.matrix\n",
    "        out = tensor(out_matrix, (self, other), '*')\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "        \n",
    "    def __matmul__(self, other):\n",
    "        other = other if isinstance(other, tensor) else tensor(other)\n",
    "        assert other.shape()[0] == self.shape()[-1], \"Dimension Unsupported for @\"\n",
    "        out_matrix = self.matrix @ other.matrix\n",
    "        out = tensor(out_matrix, (self, other), '@')\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def transpose(self):\n",
    "        out_matrix = self.matrix.transpose()\n",
    "        out = tensor(out_matrix, (self, ), 'T')\n",
    "\n",
    "        return out\n",
    "\n",
    "    def checkOther(self, other):\n",
    "        if isinstance(other, int | float):\n",
    "            other = tensor.const(self.rows, self.columns, other)\n",
    "        elif not isinstance(other, tensor):\n",
    "            other = tensor(other)\n",
    "        assert other.shape() == self.shape(), \"Operand Tensor sizes dont match\"\n",
    "\n",
    "        return other\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d24bcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "t = tensor()\n",
    "print(t.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3946c48a",
   "metadata": {},
   "source": [
    "Now let us add the backward lamda functions for each operation. This would be a headache. Either you can first derive them on paper yourself based on the concepts of Jacobians, downstream/upstream/local gradients, tensor contaction etc. or use standard formula. The first one is recommended for sound understanding. For reference, there are notes present in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a80e0f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensor:\n",
    "    def __init__(self, fromArray=np.zeros((2,2)), _children = (), _operation = ''):\n",
    "        fromArray = fromArray if isinstance(fromArray, np.ndarray) else np.array(fromArray)\n",
    "        assert len(fromArray.shape) == 2, \"Only 2D Tensors or Scalar to 2D Supported!\"\n",
    "        self.matrix = fromArray\n",
    "        self.rows = fromArray.shape[0]\n",
    "        self.columns = fromArray.shape[1]\n",
    "        self._prev = set(_children)\n",
    "        self._operation = _operation\n",
    "        self._backward = lambda : None\n",
    "        self.grad = None\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor Values = {self.matrix}\"\n",
    "    \n",
    "    @classmethod\n",
    "    def zeros(cls, rows, columns, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = np.zeros((rows, columns), dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls, rows, columns, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.random.rand(rows, columns)).astype(dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def const(cls, rows, columns, constant=1, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.full((rows, columns), constant)).astype(dtype=dtype)\n",
    "        t.rows = rows\n",
    "        t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    #Operations\n",
    "    def __add__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix + other.matrix\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(out.grad) if other.grad is None else other.grad\n",
    "            self.grad += out.grad #Derivation in the notes. \n",
    "            other.grad += out.grad\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return self + (-1 * other)\n",
    "    \n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return other + (-1 * other)\n",
    "    \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix * other.matrix\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(out.grad) if other.grad is None else other.grad\n",
    "            self.grad += out.grad * other.matrix #Derivation in the notes. \n",
    "            other.grad += out.grad * self.matrix\n",
    "\n",
    "        out = tensor(out_matrix, (self, other), '*')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    '''\n",
    "    batch multiplication might cause shape broadcasts.\n",
    "    eg. (3,2,2) @ (1,2,3) = (3,2,3)\n",
    "    this is similar to our element wise operations\n",
    "    thus we should be handling this the same way we did for elementwise operations\n",
    "    But, for now, we would be working in a controlled way (Even for CNNS)\n",
    "    and wouldn't need this handling.\n",
    "    '''\n",
    "    def __matmul__(self, other):\n",
    "        other = other if isinstance(other, tensor) else tensor(other)\n",
    "        assert other.shape()[0] == self.shape()[-1], \"Dimension Unsupported for @\"\n",
    "        out_matrix = self.matrix @ other.matrix\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(other.matrix) if other.grad is None else other.grad\n",
    "            self.grad += out.grad @ (other.matrix).swapaxes(-2,-1)#Derivation in the notes.\n",
    "            other.grad += (self.matrix).swapaxes(-2,-1) @ out.grad \n",
    "        out = tensor(out_matrix, (self, other), '@')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "\n",
    "    #I and thus we should learn at this point that to make our class compatible for ND tensors,\n",
    "    #We need the matrix multiplication and Transpose backward to change\n",
    "    #For higher dimensions, matmul = batch matmul where multiplication is done \n",
    "    #along each and every batches of 2D matrix. \n",
    "    #eg. If we have (2,3,3) shape tensor, it implies there are two batches of (3,3) matrices\n",
    "    #similarly, (2,3,3,2) shape = 2x3 batches of 3x2 matrices.\n",
    "    #matrix multiplication, (2,3,3) @ (2,3,2) = (2,3,2)\n",
    "    def swap_axes(self, axis1, axis2):\n",
    "        out_matrix = self.matrix.swapaxes(axis1, axis2)\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad.swapaxes(axis1,axis2)) if self.grad is None else self.grad\n",
    "            self.grad += (out.grad).swapaxes(axis1,axis2) #Not in note, but can be derived similarly.\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), 'T')\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    def transpose(self):\n",
    "        out_matrix = self.matrix.transpose()\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad.transpose()) if self.grad is None else self.grad\n",
    "            self.grad += (out.grad).transpose() #Not in note, but can be derived similarly.\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), 'T')\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, N):\n",
    "        assert isinstance(N, int | float), \"Can only power up by scalars!\"\n",
    "        out_matrix = self.matrix ** N\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += N * (self.matrix ** (N-1)) * out.grad\n",
    "        \n",
    "        out = tensor(out_matrix, _children=(self, ), _operation=\"**\")\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return self * (other**-1)\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return other * (self**-1)\n",
    "    \n",
    "    def sum(self):\n",
    "        out_matrix = np.array(([[self.matrix.sum()]]))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += np.ones_like(self.matrix) * out.grad\n",
    "\n",
    "        out = tensor(out_matrix, _children=(self, ), _operation='sum()')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def mean(self):\n",
    "        N = self.rows * self.columns\n",
    "        out_matrix = np.array(([[self.matrix.sum()/(N)]]))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += np.ones_like(self.matrix) * out.grad / N\n",
    "\n",
    "        out = tensor(out_matrix, _children=(self, ), _operation='mean()')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def ReLU(self):\n",
    "        out_matrix = np.maximum(0,self.matrix)\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += (self.matrix > 0).astype(self.matrix.dtype) * out.grad\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), \"ReLU\")\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    #Helper Functions\n",
    "    def shape(self):\n",
    "        return (self.rows, self.columns)\n",
    "    \n",
    "    def checkOther(self, other):\n",
    "        if isinstance(other, int | float):\n",
    "            other = tensor.const(self.rows, self.columns, other)\n",
    "        elif not isinstance(other, tensor):\n",
    "            other = tensor(other)\n",
    "        assert other.shape() == self.shape(), \"Operand Tensor sizes dont match\"\n",
    "\n",
    "        return other\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad = None\n",
    "        \n",
    "    def backward(self):\n",
    "        self.grad = np.ones_like(self.matrix)\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        for current in reversed(topo):\n",
    "\n",
    "            current._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae85391f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor Values = [[2.00000006 7.00000021]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tensor([[2.0, 3.0]])\n",
    "b = tensor([[1.0, 4.0]])\n",
    "c = a + b\n",
    "d = c * a\n",
    "d = d / 3\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef129518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Tensor Values = [[1. 2.]\n",
      " [3. 4.]]\n",
      "True: Tensor Values = [[1.5 2.5]\n",
      " [2.5 3.5]]\n",
      "Loss: 1.0\n",
      "Gradients for Y_pred:\n",
      "[[-1. -1.]\n",
      " [ 1.  1.]]\n",
      "Gradients for Y_true:\n",
      "[[ 1.  1.]\n",
      " [-1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "Y_true = tensor([[1.5, 2.5], [2.5, 3.5]])\n",
    "\n",
    "loss = ((Y_pred - Y_true) ** 2)\n",
    "print(\"Predicted:\", Y_pred)\n",
    "print(\"True:\", Y_true)\n",
    "print(\"Loss:\", loss.matrix.sum())\n",
    "\n",
    "loss.backward()\n",
    "print(\"Gradients for Y_pred:\")\n",
    "print(Y_pred.grad)\n",
    "print(\"Gradients for Y_true:\")\n",
    "print(Y_true.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169c326b",
   "metadata": {},
   "source": [
    "Now lets try to make it support all D tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "afb888f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensor:\n",
    "    def __init__(self, fromArray=np.zeros((2,2)), _children = (), _operation = ''):\n",
    "        fromArray = fromArray if isinstance(fromArray, np.ndarray) else np.array(fromArray)\n",
    "        #assert len(fromArray.shape) == 2, \"Only 2D Tensors or Scalar to 2D Supported!\"\n",
    "        self.matrix = fromArray\n",
    "        #self.rows = fromArray.shape[0]\n",
    "        #self.columns = fromArray.shape[1]\n",
    "        self.shape = fromArray.shape\n",
    "        self._prev = set(_children)\n",
    "        self._operation = _operation\n",
    "        self._backward = lambda : None\n",
    "        self.grad = None\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor Values = {self.matrix}\"\n",
    "    \n",
    "    @classmethod\n",
    "    def zeros(cls, shape, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = np.zeros(shape, dtype=dtype)\n",
    "        t.shape = shape\n",
    "        #t.rows = rows\n",
    "        #t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls, shape, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.random.rand(*shape)).astype(dtype=dtype)\n",
    "        t.shape = shape\n",
    "        #t.rows = rows\n",
    "        #t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def const(cls, shape, constant=1, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.full(shape, constant)).astype(dtype=dtype)\n",
    "        t.shape = shape\n",
    "        #t.rows = rows\n",
    "        #t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    #Operations\n",
    "    def __add__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix + other.matrix\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(out.grad) if other.grad is None else other.grad\n",
    "            self.grad += out.grad #Derivation in the notes. \n",
    "            other.grad += out.grad\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return self + (-1 * other)\n",
    "    \n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return other + (-1 * other)\n",
    "    \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix * other.matrix\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(out.grad) if other.grad is None else other.grad\n",
    "            self.grad += out.grad * other.matrix #Derivation in the notes. \n",
    "            other.grad += out.grad * self.matrix\n",
    "\n",
    "        out = tensor(out_matrix, (self, other), '*')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    '''\n",
    "    batch multiplication might cause shape broadcasts.\n",
    "    eg. (3,2,2) @ (1,2,3) = (3,2,3)\n",
    "    this is similar to our element wise operations\n",
    "    thus we should be handling this the same way we did for elementwise operations\n",
    "    But, for now, we would be working in a controlled way (Even for CNNS)\n",
    "    and wouldn't need this handling.\n",
    "    '''\n",
    "    def __matmul__(self, other):\n",
    "        other = other if isinstance(other, tensor) else tensor(other)\n",
    "        assert other.shape()[0] == self.shape()[-1], \"Dimension Unsupported for @\"\n",
    "        out_matrix = self.matrix @ other.matrix\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(other.matrix) if other.grad is None else other.grad\n",
    "            self.grad += out.grad @ (other.matrix).swapaxes(-2,-1)#Derivation in the notes.\n",
    "            other.grad += (self.matrix).swapaxes(-2,-1) @ out.grad \n",
    "        out = tensor(out_matrix, (self, other), '@')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "\n",
    "    #I and thus we should learn at this point that to make our class compatible for ND tensors,\n",
    "    #We need the matrix multiplication and Transpose backward to change\n",
    "    #For higher dimensions, matmul = batch matmul where multiplication is done \n",
    "    #along each and every batches of 2D matrix. \n",
    "    #eg. If we have (2,3,3) shape tensor, it implies there are two batches of (3,3) matrices\n",
    "    #similarly, (2,3,3,2) shape = 2x3 batches of 3x2 matrices.\n",
    "    #matrix multiplication, (2,3,3) @ (2,3,2) = (2,3,2)\n",
    "    def swap_axes(self, axis1, axis2):\n",
    "        out_matrix = self.matrix.swapaxes(axis1, axis2)\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad.swapaxes(axis1,axis2)) if self.grad is None else self.grad\n",
    "            self.grad += (out.grad).swapaxes(axis1,axis2) #Not in note, but can be derived similarly.\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), 'T')\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    def transpose(self):\n",
    "        out_matrix = self.matrix.transpose()\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad.transpose()) if self.grad is None else self.grad\n",
    "            self.grad += (out.grad).transpose() #Not in note, but can be derived similarly.\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), 'T')\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, N):\n",
    "        assert isinstance(N, int | float), \"Can only power up by scalars!\"\n",
    "        out_matrix = self.matrix ** N\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += N * (self.matrix ** (N-1)) * out.grad\n",
    "        \n",
    "        out = tensor(out_matrix, _children=(self, ), _operation=\"**\")\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return self * (other**-1)\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return other * (self**-1)\n",
    "    \n",
    "    def sum(self):\n",
    "        out_matrix = np.array(([[self.matrix.sum()]]))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += np.ones_like(self.matrix) * out.grad\n",
    "\n",
    "        out = tensor(out_matrix, _children=(self, ), _operation='sum()')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def mean(self):\n",
    "        N = np.prod(self.shape)\n",
    "        out_matrix = np.array(([[self.matrix.sum()/(N)]]))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += np.ones_like(self.matrix) * out.grad / N\n",
    "\n",
    "        out = tensor(out_matrix, _children=(self, ), _operation='mean()')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def ReLU(self):\n",
    "        out_matrix = np.maximum(0,self.matrix)\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += (self.matrix > 0).astype(self.matrix.dtype) * out.grad\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), \"ReLU\")\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    #Helper Functions\n",
    "    #def shape(self):\n",
    "     #   return (self.rows, self.columns)\n",
    "    \n",
    "    def checkOther(self, other):\n",
    "        if isinstance(other, int | float):\n",
    "            other = tensor.const(self.shape, other)\n",
    "        elif not isinstance(other, tensor):\n",
    "            other = tensor(other)\n",
    "        assert other.shape == self.shape, \"Operand Tensor sizes dont match\"\n",
    "\n",
    "        return other\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad = None\n",
    "        \n",
    "    def backward(self):\n",
    "        self.grad = np.ones_like(self.matrix)\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        for current in reversed(topo):\n",
    "\n",
    "            current._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8985d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20097739, 0.70269245],\n",
       "       [0.58803901, 0.56874465]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2,2)\n",
    "y = np.random.rand(*shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ce60b20",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m tensor([[[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m      2\u001b[0m             [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m]]])\n\u001b[1;32m      3\u001b[0m b \u001b[38;5;241m=\u001b[39m tensor([[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      4\u001b[0m             [\u001b[38;5;241m2\u001b[39m]])\n\u001b[0;32m----> 5\u001b[0m a\u001b[38;5;129m@b\u001b[39m\n",
      "Cell \u001b[0;32mIn[48], line 98\u001b[0m, in \u001b[0;36mtensor.__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__matmul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m     97\u001b[0m     other \u001b[38;5;241m=\u001b[39m other \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, tensor) \u001b[38;5;28;01melse\u001b[39;00m tensor(other)\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m other\u001b[38;5;241m.\u001b[39mshape()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimension Unsupported for @\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     out_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatrix \u001b[38;5;241m@\u001b[39m other\u001b[38;5;241m.\u001b[39mmatrix\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_backward\u001b[39m():\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "a = tensor([[[1,2],\n",
    "            [2,3]]])\n",
    "b = tensor([[1],\n",
    "            [2]])\n",
    "a@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "88419a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensor:\n",
    "    def __init__(self, fromArray=np.zeros((2,2)), _children = (), _operation = ''):\n",
    "        fromArray = fromArray if isinstance(fromArray, np.ndarray) else np.array(fromArray)\n",
    "        #assert len(fromArray.shape) == 2, \"Only 2D Tensors or Scalar to 2D Supported!\"\n",
    "        self.matrix = fromArray\n",
    "        #self.rows = fromArray.shape[0]\n",
    "        #self.columns = fromArray.shape[1]\n",
    "        self.shape = fromArray.shape\n",
    "        self._prev = set(_children)\n",
    "        self._operation = _operation\n",
    "        self._backward = lambda : None\n",
    "        self.grad = None\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor Values = {self.matrix}\"\n",
    "    \n",
    "    @classmethod\n",
    "    def zeros(cls, shape, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = np.zeros(shape, dtype=dtype)\n",
    "        t.shape = shape\n",
    "        #t.rows = rows\n",
    "        #t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def random(cls, shape, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.random.randn(*shape) * 0.1).astype(dtype=dtype)\n",
    "        t.shape = shape\n",
    "        return t\n",
    "    \n",
    "    @classmethod\n",
    "    def const(cls, shape, constant=1, dtype = np.float32):\n",
    "        t = tensor()\n",
    "        t.matrix = (np.full(shape, constant)).astype(dtype=dtype)\n",
    "        t.shape = shape\n",
    "        #t.rows = rows\n",
    "        #t.columns = columns\n",
    "        return t\n",
    "    \n",
    "    #Operations\n",
    "    def __add__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix + other.matrix\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(other.matrix) if other.grad is None else other.grad\n",
    "            out1 = self.return_unbroadcasted(out)\n",
    "            out2 = other.return_unbroadcasted(out)\n",
    "            self.grad += out1 #Derivation in the notes. \n",
    "            other.grad += out2\n",
    "        out = tensor(out_matrix, (self, other), '+')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return self + (-1 * other)\n",
    "    \n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return other + (-1 * other)\n",
    "    \n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        out_matrix = self.matrix * other.matrix\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(out.grad) if other.grad is None else other.grad\n",
    "            out1 = self.return_unbroadcasted(out)\n",
    "            out2 = other.return_unbroadcasted(out)\n",
    "            self.grad += out1* other.matrix #Derivation in the notes. \n",
    "            other.grad += out2 * self.matrix\n",
    "\n",
    "        out = tensor(out_matrix, (self, other), '*')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    '''\n",
    "    batch multiplication might cause shape broadcasts.\n",
    "    eg. (3,2,2) @ (1,2,3) = (3,2,3)\n",
    "    this is similar to our element wise operations\n",
    "    thus we should be handling this the same way we did for elementwise operations\n",
    "    But, for now, we would be working in a controlled way (Even for CNNS)\n",
    "    and wouldn't need this handling.\n",
    "    '''\n",
    "    def __matmul__(self, other):\n",
    "        other = other if isinstance(other, tensor) else tensor(other)\n",
    "        assert other.shape()[0] == self.shape()[-1], \"Dimension Unsupported for @\"\n",
    "        out_matrix = self.matrix @ other.matrix\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            other.grad = np.zeros_like(other.matrix) if other.grad is None else other.grad\n",
    "            self.grad += out.grad @ (other.matrix).swapaxes(-2,-1)#Derivation in the notes.\n",
    "            other.grad += (self.matrix).swapaxes(-2,-1) @ out.grad \n",
    "        out = tensor(out_matrix, (self, other), '@')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "\n",
    "    #I and thus we should learn at this point that to make our class compatible for ND tensors,\n",
    "    #We need the matrix multiplication and Transpose backward to change\n",
    "    #For higher dimensions, matmul = batch matmul where multiplication is done \n",
    "    #along each and every batches of 2D matrix. \n",
    "    #eg. If we have (2,3,3) shape tensor, it implies there are two batches of (3,3) matrices\n",
    "    #similarly, (2,3,3,2) shape = 2x3 batches of 3x2 matrices.\n",
    "    #matrix multiplication, (2,3,3) @ (2,3,2) = (2,3,2)\n",
    "    def swap_axes(self, axis1, axis2):\n",
    "        out_matrix = self.matrix.swapaxes(axis1, axis2)\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad.swapaxes(axis1,axis2)) if self.grad is None else self.grad\n",
    "            self.grad += (out.grad).swapaxes(axis1,axis2) #Not in note, but can be derived similarly.\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), 'T')\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def transpose(self):\n",
    "        out_matrix = self.matrix.transpose()\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(out.grad.transpose()) if self.grad is None else self.grad\n",
    "            self.grad += (out.grad).transpose() #Not in note, but can be derived similarly.\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), 'T')\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __rmatmul__(self, other):\n",
    "        other = other if isinstance(other, tensor) else tensor(other)\n",
    "        return other @ self\n",
    "    \n",
    "    def __pow__(self, N):\n",
    "        assert isinstance(N, int | float), \"Can only power up by scalars!\"\n",
    "        out_matrix = self.matrix ** N\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            out1 = self.return_unbroadcasted(out)\n",
    "            self.grad += N * (self.matrix ** (N-1)) * out1\n",
    "        \n",
    "        out = tensor(out_matrix, _children=(self, ), _operation=\"**\")\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        other = self.checkOther(other)\n",
    "        return self * (other**-1)\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return other * (self**-1)\n",
    "    \n",
    "    def sum(self):\n",
    "        out_matrix = np.array(([[self.matrix.sum()]]))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += np.ones_like(self.matrix) * out.grad\n",
    "\n",
    "        out = tensor(out_matrix, _children=(self, ), _operation='sum()')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def mean(self):\n",
    "        N = np.prod(self.shape)\n",
    "        out_matrix = np.array(([[self.matrix.sum()/(N)]]))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += np.ones_like(self.matrix) * out.grad / N\n",
    "\n",
    "        out = tensor(out_matrix, _children=(self, ), _operation='mean()')\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def ReLU(self):\n",
    "        out_matrix = np.maximum(0,self.matrix)\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += (self.matrix > 0).astype(self.matrix.dtype) * out.grad\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), \"ReLU\")\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def reshape(self, shape):\n",
    "        assert isinstance(shape, tuple), f\"Can only reshape using shape tuples e.g. (3,3). Provided is {shape}\"\n",
    "        out_matrix = self.matrix.reshape(shape)\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += out.grad.reshape(self.shape)\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), \"reshape()\")\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def flatten(self):\n",
    "        out_matrix = self.matrix.reshape(-1,np.prod(self.shape[1:]))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = np.zeros_like(self.matrix) if self.grad is None else self.grad\n",
    "            self.grad += out.grad.reshape(self.shape)\n",
    "\n",
    "        out = tensor(out_matrix, (self, ), \"flatten()\")\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    #Helper Functions\n",
    "    #def shape(self):\n",
    "     #   return (self.rows, self.columns)\n",
    "\n",
    "    def return_unbroadcasted(self, out):  \n",
    "        added_axis = []\n",
    "        stretched_axis = []\n",
    "        for index, (first_no, second_no) in enumerate(itertools.zip_longest(reversed(self.shape), reversed(out.shape))):\n",
    "            if first_no is None:\n",
    "                added_axis.append(index)\n",
    "            elif (first_no == 1) and (second_no > 1):\n",
    "                stretched_axis.append(index)\n",
    "        grad = out.grad\n",
    "        ndim = len(out.shape)\n",
    "        if stretched_axis:\n",
    "            original_axes = tuple(ndim - 1 - i for i in stretched_axis)\n",
    "            grad = np.sum(grad, axis=original_axes, keepdims=True)\n",
    "        if added_axis:\n",
    "            original_axes = tuple(ndim - 1 - i for i in added_axis)\n",
    "            grad = np.sum(grad, axis=original_axes, keepdims=False)\n",
    "        return grad\n",
    "\n",
    "    def checkOther(self, other):\n",
    "        if isinstance(other, int | float):\n",
    "            other = tensor.const(self.shape, other)\n",
    "        elif not isinstance(other, tensor):\n",
    "            other = tensor(other)\n",
    "        #assert other.shape == self.shape, \"Operand Tensor sizes dont match\"\n",
    "\n",
    "        return other\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad = None\n",
    "        \n",
    "    def backward(self):\n",
    "        self.grad = np.ones_like(self.matrix, dtype=float)\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        for current in reversed(topo):\n",
    "\n",
    "            current._backward()\n",
    "\n",
    "    __array_ufunc__ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cf7564a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  (2, 4)\n",
      "[[11 12 13 14]\n",
      " [21 22 23 24]]\n",
      "After:  (2, 2, 2)\n",
      "[[[11 12]\n",
      "  [13 14]]\n",
      "\n",
      " [[21 22]\n",
      "  [23 24]]]\n",
      "After2:  (2, 2, 2)\n",
      "[[[11 12]\n",
      "  [13 14]]\n",
      "\n",
      " [[21 22]\n",
      "  [23 24]]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[11, 12, 13, 14],\n",
    "             [21, 22, 23, 24]])\n",
    "\n",
    "print(\"Before: \", X.shape)\n",
    "print(X)\n",
    "X = X.reshape(2,2,2)\n",
    "print(\"After: \", X.shape)\n",
    "print(X)\n",
    "X = X.reshape(-1,*X.shape[1:])\n",
    "print(\"After2: \", X.shape)\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1f35170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2048)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tensor.random((4,32,8,8))\n",
    "y = x.flatten()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93710ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
