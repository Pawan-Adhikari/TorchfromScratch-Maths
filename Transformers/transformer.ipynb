{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df96525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from Tensorlib import tensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b8ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloc = Path(\"./Data/input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "534e8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (dataloc, 'r', encoding='utf-8') as datafile:\n",
    "    data = datafile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f64ab40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06033338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 65\n",
      "Vocabulary: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(data))\n",
    "print(f\"Vocabulary Size: {len(vocab)}\")\n",
    "print(f\"Vocabulary: {vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "155a30e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenizer:\n",
    "    def __init__(self, mode = 'char', max_vocab_size=None):\n",
    "        self.mode = mode\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "\n",
    "        self.vocab = set()\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "\n",
    "    def train(self, dataset):\n",
    "        if self.mode == 'char':\n",
    "            self.vocab = sorted(set(dataset))\n",
    "            self.stoi = {c:i for i, c in enumerate(self.vocab)}\n",
    "            self.itos = {i:c for i, c in enumerate(self.vocab)}\n",
    "        elif self.mode == 'word':\n",
    "            self.vocab = sorted(set(dataset.split()))\n",
    "            self.stoi = {s:i for i, s in enumerate(self.vocab)}\n",
    "            self.itos = {i:s for i, s in enumerate(self.vocab)}\n",
    "        else:\n",
    "            print(f\"{self.mode} not supported yet! We only support 'char' and 'word'!\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        if len(self.stoi) == 0:\n",
    "            raise ValueError(\"First run train method on your data!\")\n",
    "        \n",
    "        if self.mode == 'char':\n",
    "            return [self.stoi[c] for c in text]\n",
    "        elif self.mode == 'word':\n",
    "            return [self.stoi[c] for c in text.split()]\n",
    "        else:\n",
    "            raise ValueError(f\"{self.mode} not supported yet! We only support 'char' and 'word'!\")\n",
    "\n",
    "    def decode(self, ints):\n",
    "        if len(self.itos) == 0:\n",
    "            raise ValueError(\"First run train method on your data!\")\n",
    "        \n",
    "        tlists = [self.itos[i] for i in ints]\n",
    "        if self.mode == 'char':\n",
    "            return \"\".join(tlists)\n",
    "        elif self.mode == 'word':\n",
    "            return \" \".join(tlists)\n",
    "        else: \n",
    "            raise ValueError(f\"{self.mode} not supported yet! We only support 'char' and 'word'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41c965cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42, 2, 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizer('char')\n",
    "t.train(data)\n",
    "encoded = t.encode(\"Hello World!!\")\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db341f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello World!!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = t.decode(encoded)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20aed0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class embeddings:\n",
    "    def __init__(self, vocab_size=65, embedding_dim=65):\n",
    "        self.table = tensor.random(shape=(vocab_size, embedding_dim), dtype=np.float32)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def __call__(self, target): #Target = tensor of shape: Batch_size x context_size. eg. 16x10 for 10 words context\n",
    "                                #out_matrix = np.zeros(shape=(target.shape[0], target.shape[1], self.embedding_dim))\n",
    "\n",
    "        '''for batch_idx, i in enumerate(target):\n",
    "            for token_idx, j in enumerate(i):\n",
    "                out_matrix[batch_idx][token_idx] = self.table.matrix[j]'''\n",
    "        #Need to vectorise this^^\n",
    "        #how ? Fancy indexing!\n",
    "        # 1)\n",
    "        out_matrix = self.table.matrix[target.matrix] #Fancy indexing\n",
    "\n",
    "\n",
    "        def _backward(grad):\n",
    "            if self.table.grad is None:\n",
    "                self.table.grad = np.zeros_like(self.table.matrix, dtype=np.float32)\n",
    "            np.add.at(self.table.grad, target.matrix.ravel(), grad.reshape(-1, self.embedding_dim))\n",
    "\n",
    "        out = tensor(out_matrix, _children=(self.table, ), _operation='Embedding_Lookup')\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a352d6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor Values = [[-0.13355795 -0.02545915]\n",
       " [ 0.18224415  0.01131034]\n",
       " [ 0.06942736 -0.2055572 ]\n",
       " [-0.08420317  0.12346265]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1)\n",
    "table = tensor.random(shape=(4, 2), dtype=np.float32)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747fcb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target.shape: (3, 3)\n",
      "out_matrix.shape: (3, 3, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.18224415,  0.01131034],\n",
       "        [-0.13355795, -0.02545915],\n",
       "        [-0.08420317,  0.12346265]],\n",
       "\n",
       "       [[ 0.18224415,  0.01131034],\n",
       "        [ 0.18224415,  0.01131034],\n",
       "        [ 0.06942736, -0.2055572 ]],\n",
       "\n",
       "       [[ 0.06942736, -0.2055572 ],\n",
       "        [-0.08420317,  0.12346265],\n",
       "        [-0.13355795, -0.02545915]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = tensor([[1, 0, 3],\n",
    "                [1, 1, 2],\n",
    "                [2, 3, 0]])\n",
    "\n",
    "print(f\"target.shape: {target.shape}\")\n",
    "\n",
    "out_matrix = table.matrix[target.matrix] #Fancy indexing\n",
    "\n",
    "print(f\"out_matrix.shape: {out_matrix.shape}\")\n",
    "out_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3696e8ac",
   "metadata": {},
   "source": [
    "## Test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fedf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: (2, 3, 4)\n",
      "Gradient Shape: (10, 4)\n",
      "Gradient at Index 1 (should be 2.0): [2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "embed_dim = 4\n",
    "\n",
    "# 1. Init Layer\n",
    "emb = embeddings(vocab_size, embed_dim)\n",
    "\n",
    "# 2. (Batch=2, Context=3)\n",
    "\n",
    "input_data = np.array([[1, 4, 1], [0, 9, 2]]) \n",
    "input_tensor = tensor(input_data) \n",
    "\n",
    "# 3. Forward Pass\n",
    "output = emb(input_tensor)\n",
    "print(\"Output Shape:\", output.matrix.shape) \n",
    "# Expected: (2, 3, 4)\n",
    "\n",
    "# 4. Backward Pass (Fake Gradients)\n",
    "fake_grad = np.ones((2, 3, 4)) # Passing a gradient of 1.0 everywhere\n",
    "output._backward(fake_grad) \n",
    "\n",
    "print(\"Gradient Shape:\", emb.table.grad.shape)\n",
    "# Expected: (10, 4)\n",
    "print(\"Gradient at Index 1:\", emb.table.grad[1])\n",
    "# Because Index '1' appears twice in the input [[1, 4, 1]...], \n",
    "# its gradient should be 1.0 + 1.0 = 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70db1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
